# Spark EventLog è§£æå™¨ (PySparkç‰ˆæœ¬)

åŸºäºPySparkçš„å¤§è§„æ¨¡EventLogè§£ææ–¹æ¡ˆï¼Œæ”¯æŒæ¯å¤©30ä¸‡+ä»»åŠ¡çš„è§£æï¼Œå°†è§£æç»“æœå­˜å‚¨åˆ°Hiveæ•°æ®ä»“åº“ã€‚

## âš¡ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚

- Spark 3.x (PySpark)
- Hadoop 3.x (HDFS)
- Hive 3.x
- Python 3.7+
- PyYAML 5.4+

### 2. é…ç½®æ–‡ä»¶

å¤åˆ¶é…ç½®æ–‡ä»¶æ¨¡æ¿å¹¶ä¿®æ”¹ï¼š

```bash
cp config.yaml.example config.yaml
vim config.yaml
```

å…³é”®é…ç½®é¡¹ï¼š

```yaml
hdfs:
  clusters:
    cluster1:
      event_log_dir: "/spark-logs"  # ä¿®æ”¹ä¸ºå®é™…è·¯å¾„
      use_date_subdir: true          # æ˜¯å¦æŒ‰æ—¥æœŸå­ç›®å½•ç»„ç»‡

hive:
  database: "meta"                   # Hiveåº“å

parser:
  parse_parallelism: 2000            # å¹¶è¡Œåº¦
  skip_inprogress: true              # è·³è¿‡è¿è¡Œä¸­çš„ä»»åŠ¡
```

### 3. å®‰è£…Pythonä¾èµ–

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æˆ–è€…å®‰è£…ä¸ºPythonåŒ…
pip install -e .
```

### 4. åˆ›å»ºHiveè¡¨

```bash
# æ‰§è¡Œå»ºè¡¨SQL
hive -f create_hive_tables.sql
```

ä¸»è¦è¡¨ï¼š
- `meta.spark_applications` - åº”ç”¨çº§åˆ«æŒ‡æ ‡
- `meta.spark_stages` - Stageçº§åˆ«æŒ‡æ ‡
- `meta.spark_executors` - Executorä¿¡æ¯
- `meta.spark_diagnosis` - è¯Šæ–­å»ºè®®
- `meta.spark_parser_status` - è§£æçŠ¶æ€

### 5. æäº¤ä»»åŠ¡

#### æ–¹å¼1ï¼šä½¿ç”¨æäº¤è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# è§£ææ˜¨å¤©çš„æ—¥å¿—
./submit_parser.sh cluster1

# è§£ææŒ‡å®šæ—¥æœŸ
./submit_parser.sh cluster1 2024-01-15

# è‡ªå®šä¹‰èµ„æºé…ç½®
NUM_EXECUTORS=300 PARALLELISM=3000 ./submit_parser.sh cluster1 2024-01-15
```

#### æ–¹å¼2ï¼šç›´æ¥æäº¤ï¼ˆå¼€å‘æµ‹è¯•ï¼‰

```bash
# å…ˆæ‰“åŒ…Pythonæ¨¡å—
cd /path/to/parseSpark
zip -r parser.zip parser/
zip -r models.zip models/
zip -r utils.zip utils/

# æäº¤ä»»åŠ¡
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 8g \
  --executor-memory 12g \
  --executor-cores 4 \
  --num-executors 200 \
  --conf spark.app.cluster_name=cluster1 \
  --conf spark.app.target_date=2024-01-15 \
  --conf spark.app.config_path=./config.yaml \
  --py-files parser.zip,models.zip,utils.zip \
  parse_spark_logs.py
```

#### æ–¹å¼3ï¼šAirflowè°ƒåº¦ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

å‚è€ƒ `airflow/spark_parser_dag.py` é…ç½®DAGï¼š

```python
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

parse_task = SparkSubmitOperator(
    task_id='parse_cluster1',
    application='/path/to/parse_spark_logs.py',
    py_files='/path/to/parser.zip,/path/to/models.zip,/path/to/utils.zip',
    conf={
        'spark.app.cluster_name': 'cluster1',
        'spark.app.target_date': '{{ ds }}',
    },
    ...
)
```

## ğŸ“Š æ¶æ„è®¾è®¡

### æ ¸å¿ƒæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è°ƒåº¦ç³»ç»Ÿï¼ˆCrontab/Airflowï¼‰         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ spark-submit
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sparkä»»åŠ¡ï¼ˆAll-in-Oneï¼‰             â”‚
â”‚  1. åˆ†å¸ƒå¼æ‰«æHDFS                   â”‚
â”‚  2. å¹¶è¡Œè§£æEventLog                 â”‚
â”‚  3. æ•°æ®è´¨é‡æ ¡éªŒ                     â”‚
â”‚  4. å†™å…¥Hiveï¼ˆå¹‚ç­‰ï¼‰                 â”‚
â”‚  5. ç›‘æ§æŒ‡æ ‡ä¸ŠæŠ¥                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®ç‰¹æ€§

âœ… **çº¯Sparkæ–¹æ¡ˆ** - æ— Pythonä¸»æ§ï¼Œé¿å…å•ç‚¹ç“¶é¢ˆ  
âœ… **åˆ†å¸ƒå¼æ‰«æ** - Sparkå¹¶è¡Œæ‰«æHDFSï¼Œæ”¯æŒç™¾ä¸‡çº§æ–‡ä»¶  
âœ… **å¹‚ç­‰æ€§ä¿è¯** - INSERT OVERWRITEï¼Œæ”¯æŒä»»åŠ¡é‡è·‘  
âœ… **æ•°æ®è´¨é‡æ ¡éªŒ** - è§£æåç«‹å³éªŒè¯ï¼Œé¿å…è„æ•°æ®  
âœ… **å®¹é”™æœºåˆ¶** - å•æ–‡ä»¶å¤±è´¥ä¸å½±å“å…¨å±€  
âœ… **ç›‘æ§å‘Šè­¦** - Prometheus + Grafanaå®æ—¶ç›‘æ§  

## ğŸ”§ é…ç½®è¯´æ˜

### èµ„æºé…ç½®ï¼ˆæ ¹æ®æ–‡ä»¶æ•°é‡è°ƒæ•´ï¼‰

| æ–‡ä»¶æ•°é‡ | Executoræ•°é‡ | å¹¶è¡Œåº¦ | é¢„è®¡è€—æ—¶ |
|---------|-------------|--------|---------|
| 1,000 | 10 | 100 | 2-5åˆ†é’Ÿ |
| 10,000 | 50 | 500 | 5-10åˆ†é’Ÿ |
| 100,000 | 150 | 1500 | 15-30åˆ†é’Ÿ |
| 300,000+ | 200-500 | 2000-5000 | 20-60åˆ†é’Ÿ |

### å¹¶è¡Œåº¦è®¡ç®—å…¬å¼

```
å¹¶è¡Œåº¦ = executoræ•°é‡ Ã— executoræ ¸å¿ƒæ•° Ã— (2-3)
```

ç¤ºä¾‹ï¼š
- 200 executors Ã— 4 cores Ã— 2.5 = 2000 partitions

## ğŸ“ˆ ç›‘æ§å’Œå‘Šè­¦

### PrometheusæŒ‡æ ‡

```
# è§£ææ–‡ä»¶æ•°
spark_eventlog_parse_total{cluster="cluster1", status="success"}

# è§£æè€—æ—¶
spark_eventlog_parse_duration_seconds{cluster="cluster1"}

# åº”ç”¨æ•°é‡
spark_applications_count{cluster="cluster1", date="2024-01-15"}
```

### Grafana Dashboard

è®¿é—®ï¼š`http://grafana:3000/d/spark-parser`

ä¸»è¦é¢æ¿ï¼š
- è§£ææˆåŠŸç‡è¶‹åŠ¿
- è§£æè€—æ—¶åˆ†å¸ƒ
- æ¯æ—¥åº”ç”¨æ•°é‡
- å¤±è´¥æ–‡ä»¶åˆ—è¡¨

### å‘Šè­¦è§„åˆ™

- **è§£æå¤±è´¥ç‡è¿‡é«˜**ï¼šå¤±è´¥ç‡ > 5%
- **ä»»åŠ¡è¶…æ—¶**ï¼šè¶…è¿‡4å°æ—¶æœªå®Œæˆ
- **æ•°æ®é‡å¼‚å¸¸**ï¼šç›¸æ¯”æ˜¨å¤©å˜åŒ–è¶…è¿‡30%

## ğŸ” æ•°æ®æŸ¥è¯¢ç¤ºä¾‹

### æŸ¥è¯¢æ¯æ—¥åº”ç”¨æ•°é‡

```sql
SELECT dt, cluster_name, COUNT(*) as app_count
FROM meta.spark_applications
WHERE dt >= '2024-01-01'
GROUP BY dt, cluster_name
ORDER BY dt DESC;
```

### æŸ¥è¯¢å¤±è´¥ä»»åŠ¡

```sql
SELECT app_id, app_name, user, duration_ms
FROM meta.spark_applications
WHERE dt = '2024-01-15' 
  AND cluster_name = 'cluster1'
  AND status = 'FAILED'
ORDER BY duration_ms DESC
LIMIT 100;
```

### æŸ¥è¯¢æ•°æ®å€¾æ–œä¸¥é‡çš„Stage

```sql
SELECT app_id, stage_id, stage_name, 
       skew_factor, task_duration_p95, task_duration_max
FROM meta.spark_stages
WHERE dt = '2024-01-15'
  AND skew_factor > 5  -- æœ€æ…¢Taskæ˜¯ä¸­ä½æ•°çš„5å€ä»¥ä¸Š
ORDER BY skew_factor DESC
LIMIT 50;
```

### æŸ¥è¯¢è¯Šæ–­å»ºè®®

```sql
SELECT app_id, rule_desc, severity, diagnosis_detail, suggestion
FROM meta.spark_diagnosis
WHERE dt = '2024-01-15'
  AND severity IN ('CRITICAL', 'WARNING')
ORDER BY 
  CASE severity 
    WHEN 'CRITICAL' THEN 1
    WHEN 'WARNING' THEN 2
    ELSE 3
  END;
```

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1ï¼šä»»åŠ¡æ‰§è¡Œè¶…æ—¶

**æ’æŸ¥æ­¥éª¤ï¼š**

1. æŸ¥çœ‹Spark UIï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¡ä½çš„Task
2. æ£€æŸ¥æ–‡ä»¶æ•°é‡æ˜¯å¦è¶…å‡ºé¢„æœŸ
3. æ£€æŸ¥å¹¶è¡Œåº¦é…ç½®æ˜¯å¦åˆç†
4. æ£€æŸ¥HDFSæ˜¯å¦æœ‰æ€§èƒ½é—®é¢˜

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# å¢åŠ executoræ•°é‡å’Œå¹¶è¡Œåº¦
--num-executors 400 \
--conf spark.app.parse_parallelism=4000
```

### é—®é¢˜2ï¼šæ•°æ®é‡å¼‚å¸¸

**æ’æŸ¥æ­¥éª¤ï¼š**

```sql
-- 1. æ£€æŸ¥è§£ææ–‡ä»¶æ•°
SELECT COUNT(DISTINCT file_path) as file_count
FROM meta.spark_parser_status
WHERE dt = '2024-01-15' AND status = 'SUCCESS';

-- 2. æ£€æŸ¥åº”ç”¨æ•°é‡
SELECT COUNT(*) as app_count
FROM meta.spark_applications
WHERE dt = '2024-01-15';

-- 3. å¯¹æ¯”å‰ä¸€å¤©
SELECT dt, COUNT(*) as app_count
FROM meta.spark_applications
WHERE dt IN ('2024-01-14', '2024-01-15')
GROUP BY dt;
```

**è§£å†³æ–¹æ¡ˆï¼š**
- æ£€æŸ¥EventLogç›®å½•é…ç½®æ˜¯å¦æ­£ç¡®
- æ£€æŸ¥æ–‡ä»¶è¿‡æ»¤è§„åˆ™æ˜¯å¦å¤ªä¸¥æ ¼
- æ£€æŸ¥HDFSæƒé™

### é—®é¢˜3ï¼šå†…å­˜æº¢å‡ºï¼ˆOOMï¼‰

**æ’æŸ¥æ­¥éª¤ï¼š**

1. æŸ¥çœ‹executoræ—¥å¿—ï¼Œå®šä½OOMä½ç½®
2. æ£€æŸ¥æ˜¯å¦æœ‰è¶…å¤§æ–‡ä»¶ï¼ˆ>1GBï¼‰
3. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æµå¼è§£æ

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# å¢åŠ executorå†…å­˜
--executor-memory 16g \
--conf spark.executor.memoryOverhead=3g

# å¯ç”¨Kryoåºåˆ—åŒ–
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer
```

### é—®é¢˜4ï¼šé‡å¤æ•°æ®

**æ’æŸ¥æ­¥éª¤ï¼š**

```sql
-- æ£€æŸ¥é‡å¤è®°å½•
SELECT cluster_name, app_id, dt, COUNT(*) as cnt
FROM meta.spark_applications
WHERE dt = '2024-01-15'
GROUP BY cluster_name, app_id, dt
HAVING COUNT(*) > 1;
```

**è§£å†³æ–¹æ¡ˆï¼š**
```sql
-- æ‰‹åŠ¨å»é‡
INSERT OVERWRITE TABLE meta.spark_applications PARTITION(dt='2024-01-15')
SELECT * FROM (
  SELECT *, ROW_NUMBER() OVER(
    PARTITION BY cluster_name, app_id, dt 
    ORDER BY create_time DESC
  ) as rn
  FROM meta.spark_applications WHERE dt='2024-01-15'
) t WHERE rn = 1;
```

## ğŸ“š æ–‡æ¡£

- [è¯¦ç»†è®¾è®¡æ–‡æ¡£](./Sparkä½œä¸šè§£ææ–¹æ¡ˆè®¾è®¡.md) - å®Œæ•´çš„æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡
- [ä¼˜åŒ–æ€»ç»“](./æ–¹æ¡ˆä¼˜åŒ–æ€»ç»“.md) - ä¼˜åŒ–å»ºè®®å’Œæœ€ä½³å®è·µ
- [é…ç½®æ–‡ä»¶ç¤ºä¾‹](./config.yaml.example) - é…ç½®æ–‡ä»¶æ¨¡æ¿

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ License

MIT License

## ğŸ”— ç›¸å…³é¡¹ç›®

- [Sparkå®˜æ–¹æ–‡æ¡£](https://spark.apache.org/docs/latest/)
- [Spark History Server](https://spark.apache.org/docs/latest/monitoring.html)
- [Dr.Elephant](https://github.com/linkedin/dr-elephant) - LinkedInå¼€æºçš„Sparkæ€§èƒ½åˆ†æå·¥å…·
- [Sparklint](https://github.com/groupon/sparklint) - Grouponå¼€æºçš„Sparkæ€§èƒ½åˆ†æå·¥å…·

---

**ç»´æŠ¤è€…**: Data Platform Team  
**æœ€åæ›´æ–°**: 2024-01-15

