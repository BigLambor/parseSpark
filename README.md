# Spark EventLog Ëß£ÊûêÂô® (PySparkÁâàÊú¨)

Âü∫‰∫éPySparkÁöÑÂ§ßËßÑÊ®°EventLogËß£ÊûêÊñπÊ°àÔºåÊîØÊåÅÊØèÂ§©30‰∏á+‰ªªÂä°ÁöÑËß£ÊûêÔºåÂ∞ÜËß£ÊûêÁªìÊûúÂ≠òÂÇ®Âà∞HiveÊï∞ÊçÆ‰ªìÂ∫ì„ÄÇ

## ‚ö° Âø´ÈÄüÂºÄÂßã

### 1. ÁéØÂ¢ÉË¶ÅÊ±Ç

- Spark 3.x (PySpark)
- Hadoop 3.x (HDFS)
- Hive 3.x
- Python 3.7+
- PyYAML 5.4+

### 2. ÈÖçÁΩÆÊñá‰ª∂

Â§çÂà∂ÈÖçÁΩÆÊñá‰ª∂Ê®°ÊùøÂπ∂‰øÆÊîπÔºö

```bash
cp config.yaml.example config.yaml
vim config.yaml
```

ÂÖ≥ÈîÆÈÖçÁΩÆÈ°πÔºö

```yaml
hdfs:
  clusters:
    cluster1:
      event_log_dir: "/spark-logs"  # ‰øÆÊîπ‰∏∫ÂÆûÈôÖË∑ØÂæÑ
      use_date_subdir: true          # ÊòØÂê¶ÊåâÊó•ÊúüÂ≠êÁõÆÂΩïÁªÑÁªá

hive:
  database: "meta"                   # HiveÂ∫ìÂêç

parser:
  parse_parallelism: 2000            # Âπ∂Ë°åÂ∫¶
  skip_inprogress: true              # Ë∑≥ËøáËøêË°å‰∏≠ÁöÑ‰ªªÂä°
```

### 3. ÂÆâË£ÖPython‰æùËµñ

```bash
# ÂÆâË£Ö‰æùËµñ
pip install -r requirements.txt

# ÊàñËÄÖÂÆâË£Ö‰∏∫PythonÂåÖ
pip install -e .
```

### 4. ÂàõÂª∫HiveË°®

```bash
# ÊâßË°åÂª∫Ë°®SQL
hive -f create_hive_tables.sql
```

‰∏ªË¶ÅË°®Ôºö
- `meta.spark_applications` - Â∫îÁî®Á∫ßÂà´ÊåáÊ†á
- `meta.spark_stages` - StageÁ∫ßÂà´ÊåáÊ†á
- `meta.spark_executors` - Executor‰ø°ÊÅØ
- `meta.spark_diagnosis` - ËØäÊñ≠Âª∫ËÆÆ
- `meta.spark_parser_status` - Ëß£ÊûêÁä∂ÊÄÅ

### 5. Êèê‰∫§‰ªªÂä°

#### ÊñπÂºè1Ôºö‰ΩøÁî®Êèê‰∫§ËÑöÊú¨ÔºàÊé®ËçêÔºâ

```bash
# Ëß£ÊûêÊò®Â§©ÁöÑÊó•Âøó
./submit_parser.sh cluster1

# Ëß£ÊûêÊåáÂÆöÊó•Êúü
./submit_parser.sh cluster1 2025-12-05

# Ëá™ÂÆö‰πâËµÑÊ∫êÈÖçÁΩÆ
NUM_EXECUTORS=300 PARALLELISM=3000 ./submit_parser.sh cluster1 2025-12-05
```

#### ÊñπÂºè2ÔºöÁõ¥Êé•Êèê‰∫§ÔºàÂºÄÂèëÊµãËØïÔºâ

```bash
# ÂÖàÊâìÂåÖPythonÊ®°Âùó
cd /path/to/parseSpark
zip -r parser.zip parser/
zip -r models.zip models/
zip -r utils.zip utils/

# Êèê‰∫§‰ªªÂä°
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 8g \
  --executor-memory 12g \
  --executor-cores 4 \
  --num-executors 200 \
  --conf spark.app.cluster_name=cluster1 \
  --conf spark.app.target_date=2025-12-05 \
  --conf spark.app.config_path=./config.yaml \
  --py-files parser.zip,models.zip,utils.zip \
  parse_spark_logs.py
```

## üìä Êû∂ÊûÑËÆæËÆ°

### Ê†∏ÂøÉÊû∂ÊûÑ

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Ë∞ÉÂ∫¶Á≥ªÁªüÔºàCrontab/AirflowÔºâ         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ spark-submit
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Spark‰ªªÂä°ÔºàAll-in-OneÔºâ             ‚îÇ
‚îÇ  1. ÂàÜÂ∏ÉÂºèÊâ´ÊèèHDFS                   ‚îÇ
‚îÇ  2. Âπ∂Ë°åËß£ÊûêEventLog                 ‚îÇ
‚îÇ  3. Êï∞ÊçÆË¥®ÈáèÊ†°È™å                     ‚îÇ
‚îÇ  4. ÂÜôÂÖ•HiveÔºàÂπÇÁ≠âÔºâ                 ‚îÇ
‚îÇ  5. ÁõëÊéßÊåáÊ†á‰∏äÊä•                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### ÂÖ≥ÈîÆÁâπÊÄß

‚úÖ **Á∫ØSparkÊñπÊ°à** - Êó†Python‰∏ªÊéßÔºåÈÅøÂÖçÂçïÁÇπÁì∂È¢à  
‚úÖ **ÂàÜÂ∏ÉÂºèÊâ´Êèè** - SparkÂπ∂Ë°åÊâ´ÊèèHDFSÔºåÊîØÊåÅÁôæ‰∏áÁ∫ßÊñá‰ª∂  
‚úÖ **ÂπÇÁ≠âÊÄß‰øùËØÅ** - INSERT OVERWRITEÔºåÊîØÊåÅ‰ªªÂä°ÈáçË∑ë  
‚úÖ **Êï∞ÊçÆË¥®ÈáèÊ†°È™å** - Ëß£ÊûêÂêéÁ´ãÂç≥È™åËØÅÔºåÈÅøÂÖçËÑèÊï∞ÊçÆ  
‚úÖ **ÂÆπÈîôÊú∫Âà∂** - ÂçïÊñá‰ª∂Â§±Ë¥•‰∏çÂΩ±ÂìçÂÖ®Â±Ä  
‚úÖ **ÁõëÊéßÂëäË≠¶** - Prometheus + GrafanaÂÆûÊó∂ÁõëÊéß  

## üîß ÈÖçÁΩÆËØ¥Êòé

### ËµÑÊ∫êÈÖçÁΩÆÔºàÊ†πÊçÆÊñá‰ª∂Êï∞ÈáèË∞ÉÊï¥Ôºâ

| Êñá‰ª∂Êï∞Èáè | ExecutorÊï∞Èáè | Âπ∂Ë°åÂ∫¶ | È¢ÑËÆ°ËÄóÊó∂ |
|---------|-------------|--------|---------|
| 1,000 | 10 | 100 | 2-5ÂàÜÈíü |
| 10,000 | 50 | 500 | 5-10ÂàÜÈíü |
| 100,000 | 150 | 1500 | 15-30ÂàÜÈíü |
| 300,000+ | 200-500 | 2000-5000 | 20-60ÂàÜÈíü |

### Âπ∂Ë°åÂ∫¶ËÆ°ÁÆóÂÖ¨Âºè

```
Âπ∂Ë°åÂ∫¶ = executorÊï∞Èáè √ó executorÊ†∏ÂøÉÊï∞ √ó (2-3)
```

Á§∫‰æãÔºö
- 200 executors √ó 4 cores √ó 2.5 = 2000 partitions

## üìà ÁõëÊéßÂíåÂëäË≠¶

### PrometheusÊåáÊ†á

```
# Ëß£ÊûêÊñá‰ª∂Êï∞
spark_eventlog_parse_total{cluster="cluster1", status="success"}

# Ëß£ÊûêËÄóÊó∂
spark_eventlog_parse_duration_seconds{cluster="cluster1"}

# Â∫îÁî®Êï∞Èáè
spark_applications_count{cluster="cluster1", date="2025-12-05"}
```

### Grafana Dashboard

ËÆøÈóÆÔºö`http://grafana:3000/d/spark-parser`

‰∏ªË¶ÅÈù¢ÊùøÔºö
- Ëß£ÊûêÊàêÂäüÁéáË∂ãÂäø
- Ëß£ÊûêËÄóÊó∂ÂàÜÂ∏É
- ÊØèÊó•Â∫îÁî®Êï∞Èáè
- Â§±Ë¥•Êñá‰ª∂ÂàóË°®

### ÂëäË≠¶ËßÑÂàô

- **Ëß£ÊûêÂ§±Ë¥•ÁéáËøáÈ´ò**ÔºöÂ§±Ë¥•Áéá > 5%
- **‰ªªÂä°Ë∂ÖÊó∂**ÔºöË∂ÖËøá4Â∞èÊó∂Êú™ÂÆåÊàê
- **Êï∞ÊçÆÈáèÂºÇÂ∏∏**ÔºöÁõ∏ÊØîÊò®Â§©ÂèòÂåñË∂ÖËøá30%

## üîç Êï∞ÊçÆÊü•ËØ¢Á§∫‰æã

### Êü•ËØ¢ÊØèÊó•Â∫îÁî®Êï∞Èáè

```sql
SELECT dt, cluster_name, COUNT(*) as app_count
FROM meta.spark_applications
WHERE dt >= '2024-01-01'
GROUP BY dt, cluster_name
ORDER BY dt DESC;
```

### Êü•ËØ¢Â§±Ë¥•‰ªªÂä°

```sql
SELECT app_id, app_name, user, duration_ms
FROM meta.spark_applications
WHERE dt = '2025-12-05' 
  AND cluster_name = 'cluster1'
  AND status = 'FAILED'
ORDER BY duration_ms DESC
LIMIT 100;
```

### Êü•ËØ¢Êï∞ÊçÆÂÄæÊñú‰∏•ÈáçÁöÑStage

```sql
SELECT app_id, stage_id, stage_name, 
       skew_factor, task_duration_p95, task_duration_max
FROM meta.spark_stages
WHERE dt = '2025-12-05'
  AND skew_factor > 5  -- ÊúÄÊÖ¢TaskÊòØ‰∏≠‰ΩçÊï∞ÁöÑ5ÂÄç‰ª•‰∏ä
ORDER BY skew_factor DESC
LIMIT 50;
```

### Êü•ËØ¢ËØäÊñ≠Âª∫ËÆÆ

```sql
SELECT app_id, rule_desc, severity, diagnosis_detail, suggestion
FROM meta.spark_diagnosis
WHERE dt = '2025-12-05'
  AND severity IN ('CRITICAL', 'WARNING')
ORDER BY 
  CASE severity 
    WHEN 'CRITICAL' THEN 1
    WHEN 'WARNING' THEN 2
    ELSE 3
  END;
```

## üêõ ÊïÖÈöúÊéíÊü•

### ÈóÆÈ¢ò1Ôºö‰ªªÂä°ÊâßË°åË∂ÖÊó∂

**ÊéíÊü•Ê≠•È™§Ôºö**

1. Êü•ÁúãSpark UIÔºåÊ£ÄÊü•ÊòØÂê¶ÊúâÂç°‰ΩèÁöÑTask
2. Ê£ÄÊü•Êñá‰ª∂Êï∞ÈáèÊòØÂê¶Ë∂ÖÂá∫È¢ÑÊúü
3. Ê£ÄÊü•Âπ∂Ë°åÂ∫¶ÈÖçÁΩÆÊòØÂê¶ÂêàÁêÜ
4. Ê£ÄÊü•HDFSÊòØÂê¶ÊúâÊÄßËÉΩÈóÆÈ¢ò

**Ëß£ÂÜ≥ÊñπÊ°àÔºö**
```bash
# Â¢ûÂä†executorÊï∞ÈáèÂíåÂπ∂Ë°åÂ∫¶
--num-executors 400 \
--conf spark.app.parse_parallelism=4000
```

### ÈóÆÈ¢ò2ÔºöÊï∞ÊçÆÈáèÂºÇÂ∏∏

**ÊéíÊü•Ê≠•È™§Ôºö**

```sql
-- 1. Ê£ÄÊü•Ëß£ÊûêÊñá‰ª∂Êï∞
SELECT COUNT(DISTINCT file_path) as file_count
FROM meta.spark_parser_status
WHERE dt = '2025-12-05' AND status = 'SUCCESS';

-- 2. Ê£ÄÊü•Â∫îÁî®Êï∞Èáè
SELECT COUNT(*) as app_count
FROM meta.spark_applications
WHERE dt = '2025-12-05';

-- 3. ÂØπÊØîÂâç‰∏ÄÂ§©
SELECT dt, COUNT(*) as app_count
FROM meta.spark_applications
WHERE dt IN ('2024-01-14', '2025-12-05')
GROUP BY dt;
```

**Ëß£ÂÜ≥ÊñπÊ°àÔºö**
- Ê£ÄÊü•EventLogÁõÆÂΩïÈÖçÁΩÆÊòØÂê¶Ê≠£Á°Æ
- Ê£ÄÊü•Êñá‰ª∂ËøáÊª§ËßÑÂàôÊòØÂê¶Â§™‰∏•Ê†º
- Ê£ÄÊü•HDFSÊùÉÈôê

### ÈóÆÈ¢ò3ÔºöÂÜÖÂ≠òÊ∫¢Âá∫ÔºàOOMÔºâ

**ÊéíÊü•Ê≠•È™§Ôºö**

1. Êü•ÁúãexecutorÊó•ÂøóÔºåÂÆö‰ΩçOOM‰ΩçÁΩÆ
2. Ê£ÄÊü•ÊòØÂê¶ÊúâË∂ÖÂ§ßÊñá‰ª∂Ôºà>1GBÔºâ
3. Ê£ÄÊü•ÊòØÂê¶‰ΩøÁî®‰∫ÜÊµÅÂºèËß£Êûê

**Ëß£ÂÜ≥ÊñπÊ°àÔºö**
```bash
# Â¢ûÂä†executorÂÜÖÂ≠ò
--executor-memory 16g \
--conf spark.executor.memoryOverhead=3g

# ÂêØÁî®KryoÂ∫èÂàóÂåñ
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer
```

### ÈóÆÈ¢ò4ÔºöÈáçÂ§çÊï∞ÊçÆ

**ÊéíÊü•Ê≠•È™§Ôºö**

```sql
-- Ê£ÄÊü•ÈáçÂ§çËÆ∞ÂΩï
SELECT cluster_name, app_id, dt, COUNT(*) as cnt
FROM meta.spark_applications
WHERE dt = '2025-12-05'
GROUP BY cluster_name, app_id, dt
HAVING COUNT(*) > 1;
```

**Ëß£ÂÜ≥ÊñπÊ°àÔºö**
```sql
-- ÊâãÂä®ÂéªÈáç
INSERT OVERWRITE TABLE meta.spark_applications PARTITION(dt='2025-12-05')
SELECT * FROM (
  SELECT *, ROW_NUMBER() OVER(
    PARTITION BY cluster_name, app_id, dt 
    ORDER BY create_time DESC
  ) as rn
  FROM meta.spark_applications WHERE dt='2025-12-05'
) t WHERE rn = 1;
```
