# Spark EventLog è§£æå™¨ - æµ‹è¯•æŒ‡å—

> æœ¬æ–‡æ¡£é¢å‘æµ‹è¯•äººå‘˜ï¼ŒæŒ‡å¯¼å¦‚ä½•é…ç½®ç¯å¢ƒã€è¿è¡Œæµ‹è¯•å¹¶éªŒè¯åŠŸèƒ½ã€‚

---

## ğŸ“‹ ç›®å½•

1. [ç¯å¢ƒå‡†å¤‡](#1-ç¯å¢ƒå‡†å¤‡)
2. [é¡¹ç›®éƒ¨ç½²](#2-é¡¹ç›®éƒ¨ç½²)
3. [é…ç½®è¯´æ˜](#3-é…ç½®è¯´æ˜)
4. [æµ‹è¯•ç±»å‹](#4-æµ‹è¯•ç±»å‹)
5. [æµ‹è¯•ç”¨ä¾‹](#5-æµ‹è¯•ç”¨ä¾‹)
6. [ç»“æœéªŒè¯](#6-ç»“æœéªŒè¯)
7. [æ³¨æ„äº‹é¡¹](#7-æ³¨æ„äº‹é¡¹)
8. [å¸¸è§é—®é¢˜](#8-å¸¸è§é—®é¢˜)

---

## 1. ç¯å¢ƒå‡†å¤‡

### 1.1 å‰ç½®ä¾èµ–

| ç»„ä»¶ | ç‰ˆæœ¬è¦æ±‚ | éªŒè¯å‘½ä»¤ |
|------|----------|----------|
| Python | 3.7+ | `python3 --version` |
| Spark | 3.x | `spark-submit --version` |
| Hadoop | 3.x | `hadoop version` |
| Hive | 3.x | `hive --version` |
| YARN | 3.x | `yarn version` |

### 1.2 ç¯å¢ƒå˜é‡æ£€æŸ¥

```bash
# ç¡®è®¤ä»¥ä¸‹ç¯å¢ƒå˜é‡å·²æ­£ç¡®è®¾ç½®
echo $SPARK_HOME     # Sparkå®‰è£…ç›®å½•
echo $HADOOP_HOME    # Hadoopå®‰è£…ç›®å½•
echo $HIVE_HOME      # Hiveå®‰è£…ç›®å½•
echo $JAVA_HOME      # JDKå®‰è£…ç›®å½•
```

### 1.3 æœåŠ¡æ£€æŸ¥

```bash
# æ£€æŸ¥YARN ResourceManagerçŠ¶æ€
yarn node -list

# æ£€æŸ¥HDFSå¯ç”¨æ€§
hdfs dfsadmin -report

# æ£€æŸ¥Hive Metastore
hive -e "SHOW DATABASES"
```

### 1.4 ç½‘ç»œæƒé™

ç¡®ä¿æµ‹è¯•æœºå™¨å¯ä»¥è®¿é—®ï¼š
- HDFS NameNode ç«¯å£ï¼ˆé»˜è®¤ 8020/9000ï¼‰
- YARN ResourceManager ç«¯å£ï¼ˆé»˜è®¤ 8088ï¼‰
- Hive Metastore ç«¯å£ï¼ˆé»˜è®¤ 9083ï¼‰
- Spark History Server ç«¯å£ï¼ˆé»˜è®¤ 18080ï¼‰

---

## 2. é¡¹ç›®éƒ¨ç½²

### 2.1 è·å–ä»£ç 

```bash
# åˆ‡æ¢åˆ°éƒ¨ç½²ç›®å½•
cd /path/to/deploy

# æ‹·è´æˆ–è§£å‹é¡¹ç›®
cp -r /source/parseSpark ./
cd parseSpark
```

### 2.2 å®‰è£…Pythonä¾èµ–

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# éªŒè¯å®‰è£…
python3 -c "import yaml; print('PyYAML å®‰è£…æˆåŠŸ')"
```

### 2.3 åˆ›å»ºé…ç½®æ–‡ä»¶

```bash
# å¤åˆ¶é…ç½®æ¨¡æ¿
cp config.yaml.example config.yaml

# ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼ˆè¯¦è§ç¬¬3èŠ‚ï¼‰
vim config.yaml
```

### 2.4 åˆ›å»ºHiveè¡¨

```bash
# æ‰§è¡Œå»ºè¡¨SQL
hive -f create_hive_tables.sql

# éªŒè¯è¡¨åˆ›å»ºæˆåŠŸ
hive -e "USE meta; SHOW TABLES;"
```

**é¢„æœŸè¾“å‡ºï¼š**
```
spark_applications
spark_jobs
spark_stages
spark_executors
spark_diagnosis
spark_sql_executions
spark_configs
spark_parser_status
```

---

## 3. é…ç½®è¯´æ˜

### 3.1 å…³é”®é…ç½®é¡¹

ç¼–è¾‘ `config.yaml` æ–‡ä»¶ï¼Œä¿®æ”¹ä»¥ä¸‹å…³é”®é…ç½®ï¼š

```yaml
# HDFSé…ç½® - æ ¹æ®å®é™…é›†ç¾¤ä¿®æ”¹
hdfs:
  default_cluster: "cluster_sanqier"  # é»˜è®¤é›†ç¾¤å
  clusters:
    cluster_sanqier:
      event_log_dir: "hdfs://beh006/var/log/hadoop-spark"  # EventLogç›®å½•
      use_date_subdir: false    # æ˜¯å¦æŒ‰æ—¥æœŸå­ç›®å½•
      date_dir_format: "yyyy-MM-dd"

# Hiveé…ç½®
hive:
  database: "meta"              # Hiveæ•°æ®åº“å
  metastore_uri: "thrift://hive-metastore:9083"  # Metastoreåœ°å€

# è§£æé…ç½®
parser:
  scan_mode: "date_subdir"      # æ‰«ææ¨¡å¼
  parse_parallelism: 2000       # å¹¶è¡Œåº¦
  skip_inprogress: true         # è·³è¿‡è¿è¡Œä¸­çš„ä»»åŠ¡
```

### 3.2 é…ç½®éªŒè¯

```bash
# æ£€æŸ¥é…ç½®æ–‡ä»¶è¯­æ³•
python3 -c "import yaml; yaml.safe_load(open('config.yaml')); print('é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¡®')"

# æ£€æŸ¥EventLogç›®å½•æ˜¯å¦å¯è®¿é—®
hadoop fs -ls hdfs://beh006/var/log/hadoop-spark/ | head -5
```

### 3.3 èµ„æºé…ç½®å‚è€ƒ

| åœºæ™¯ | Executoræ•°é‡ | å†…å­˜ | å¹¶è¡Œåº¦ | é¢„è®¡è€—æ—¶ |
|------|-------------|------|--------|---------|
| å°è§„æ¨¡æµ‹è¯• (<1000æ–‡ä»¶) | 10 | 4g | 100 | 2-5åˆ†é’Ÿ |
| ä¸­è§„æ¨¡æµ‹è¯• (1ä¸‡æ–‡ä»¶) | 50 | 8g | 500 | 5-10åˆ†é’Ÿ |
| å¤§è§„æ¨¡æµ‹è¯• (10ä¸‡æ–‡ä»¶) | 150 | 12g | 1500 | 15-30åˆ†é’Ÿ |

---

## 4. æµ‹è¯•ç±»å‹

### 4.1 å•å…ƒæµ‹è¯•

#### è¿è¡Œæ–¹å¼
```bash
# æ–¹å¼1ï¼šä½¿ç”¨æµ‹è¯•è„šæœ¬
./run_tests.sh

# æ–¹å¼2ï¼šç›´æ¥è¿è¡Œpytest
export PYTHONPATH=$PYTHONPATH:$(pwd)
python -m pytest tests/ -v --tb=short
```

#### é¢„æœŸç»“æœ
```
tests/test_parser.py::TestApplicationState::test_app_metrics_conversion PASSED
tests/test_parser.py::TestApplicationState::test_job_metrics_conversion PASSED
tests/test_parser.py::TestApplicationState::test_job_status_detection PASSED
tests/test_parser.py::TestApplicationState::test_job_failure_marks_application_failed PASSED
tests/test_parser.py::TestApplicationState::test_skip_task_collection_when_disabled PASSED
tests/test_parser.py::TestMetricsCalculator::test_percentile PASSED
tests/test_parser.py::TestMetricsCalculator::test_stage_aggregates PASSED
tests/test_parser.py::TestMetricsCalculator::test_duration_calculation PASSED
======================== 8 passed in X.XXs ========================
```

### 4.2 é›†æˆæµ‹è¯•

#### æ–¹å¼1ï¼šä½¿ç”¨æäº¤è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# è§£ææ˜¨å¤©çš„æ—¥å¿—
./submit_parser.sh cluster_sanqier

# è§£ææŒ‡å®šæ—¥æœŸ
./submit_parser.sh cluster_sanqier 2025-11-26
```

#### æ–¹å¼2ï¼šå°è§„æ¨¡æµ‹è¯•ï¼ˆè°ƒæ•´èµ„æºï¼‰

```bash
# å°è§„æ¨¡æµ‹è¯•ï¼Œå‡å°‘èµ„æºæ¶ˆè€—
NUM_EXECUTORS=10 PARALLELISM=100 ./submit_parser.sh cluster_sanqier 2025-11-26
```

#### æ–¹å¼3ï¼šæ‰‹åŠ¨spark-submit

```bash
# æ‰“åŒ…Pythonæ¨¡å—
zip -r parser.zip parser/ -x "*.pyc" -x "*__pycache__*"
zip -r models.zip models/ -x "*.pyc" -x "*__pycache__*"
zip -r utils.zip utils/ -x "*.pyc" -x "*__pycache__*"

# æäº¤ä»»åŠ¡
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 4g \
  --executor-memory 4g \
  --executor-cores 2 \
  --num-executors 10 \
  --conf spark.app.cluster_name=cluster_sanqier \
  --conf spark.app.target_date=2025-11-26 \
  --conf spark.app.config_path=./config.yaml \
  --py-files parser.zip,models.zip,utils.zip \
  parse_spark_logs.py
```

---

## 5. æµ‹è¯•ç”¨ä¾‹

### 5.1 åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹

| ç”¨ä¾‹ID | ç”¨ä¾‹æè¿° | å‰ç½®æ¡ä»¶ | æµ‹è¯•æ­¥éª¤ | é¢„æœŸç»“æœ |
|--------|----------|----------|----------|----------|
| TC001 | è§£æå•æ—¥EventLog | HDFSæœ‰EventLogæ–‡ä»¶ | æ‰§è¡Œ `./submit_parser.sh cluster_sanqier 2025-11-26` | ä»»åŠ¡æˆåŠŸï¼ŒHiveæœ‰æ•°æ® |
| TC002 | è§£æç©ºç›®å½• | EventLogç›®å½•æ— æ–‡ä»¶ | æ‰§è¡Œè§£æä»»åŠ¡ | ç¨‹åºæ­£å¸¸é€€å‡ºï¼Œæç¤º"æœªæ‰¾åˆ°æ–‡ä»¶" |
| TC003 | è·³è¿‡inprogressæ–‡ä»¶ | æœ‰.inprogressæ–‡ä»¶ | æ‰§è¡Œè§£æä»»åŠ¡ | .inprogressæ–‡ä»¶è¢«è·³è¿‡ |
| TC004 | å¹¶è¡Œè§£æå¤šæ–‡ä»¶ | æœ‰100+æ–‡ä»¶ | æ‰§è¡Œè§£æä»»åŠ¡ | å¤šExecutorå¹¶è¡Œå¤„ç† |
| TC005 | å•æ–‡ä»¶è§£æå¤±è´¥ | æœ‰æŸåçš„EventLog | æ‰§è¡Œè§£æä»»åŠ¡ | è·³è¿‡åæ–‡ä»¶ï¼Œç»§ç»­å¤„ç† |
| TC006 | å¹‚ç­‰æ€§æµ‹è¯• | å·²è§£æè¿‡æ•°æ® | é‡å¤æ‰§è¡Œç›¸åŒä»»åŠ¡ | æ•°æ®è¦†ç›–ï¼Œæ— é‡å¤ |

### 5.2 æ€§èƒ½æµ‹è¯•ç”¨ä¾‹

| ç”¨ä¾‹ID | ç”¨ä¾‹æè¿° | æµ‹è¯•æ•°æ®é‡ | èµ„æºé…ç½® | éªŒæ”¶æ ‡å‡† |
|--------|----------|------------|----------|----------|
| PT001 | å°è§„æ¨¡æ€§èƒ½ | 1000æ–‡ä»¶ | 10 executors | <5åˆ†é’Ÿ |
| PT002 | ä¸­è§„æ¨¡æ€§èƒ½ | 10000æ–‡ä»¶ | 50 executors | <15åˆ†é’Ÿ |
| PT003 | å¤§è§„æ¨¡æ€§èƒ½ | 100000æ–‡ä»¶ | 150 executors | <30åˆ†é’Ÿ |

### 5.3 å¼‚å¸¸æµ‹è¯•ç”¨ä¾‹

| ç”¨ä¾‹ID | ç”¨ä¾‹æè¿° | æµ‹è¯•æ­¥éª¤ | é¢„æœŸç»“æœ |
|--------|----------|----------|----------|
| ET001 | é…ç½®æ–‡ä»¶ç¼ºå¤± | åˆ é™¤config.yamlåè¿è¡Œ | ç¨‹åºæŠ¥é”™é€€å‡ºï¼Œæç¤ºé…ç½®ç¼ºå¤± |
| ET002 | HDFSè¿æ¥å¤±è´¥ | é…ç½®é”™è¯¯çš„HDFSåœ°å€ | ç¨‹åºæŠ¥é”™ï¼Œæç¤ºè¿æ¥å¤±è´¥ |
| ET003 | Hiveåº“ä¸å­˜åœ¨ | é…ç½®ä¸å­˜åœ¨çš„æ•°æ®åº“å | ç¨‹åºæŠ¥é”™ï¼Œæç¤ºæ•°æ®åº“ä¸å­˜åœ¨ |
| ET004 | æƒé™ä¸è¶³ | ä½¿ç”¨æ— æƒé™ç”¨æˆ· | ç¨‹åºæŠ¥é”™ï¼Œæç¤ºæƒé™ä¸è¶³ |

---

## 6. ç»“æœéªŒè¯

### 6.1 ä»»åŠ¡çŠ¶æ€æ£€æŸ¥

```bash
# æŸ¥çœ‹YARNåº”ç”¨çŠ¶æ€
yarn application -list -appStates ALL | grep SparkEventLogParser

# æŸ¥çœ‹Spark History Serverï¼ˆå¦‚æœå¯ç”¨ï¼‰
# è®¿é—®: http://your-spark-history-server:18080
```

### 6.2 Hiveæ•°æ®éªŒè¯

#### æ£€æŸ¥åº”ç”¨è¡¨æ•°æ®
```sql
-- æŸ¥çœ‹è®°å½•æ•°
SELECT COUNT(*) AS app_count 
FROM meta.spark_applications 
WHERE dt = '2025-11-26' AND cluster_name = 'cluster_sanqier';

-- æŸ¥çœ‹æ•°æ®æ ·æœ¬
SELECT app_id, app_name, user, status, duration_ms
FROM meta.spark_applications
WHERE dt = '2025-11-26' AND cluster_name = 'cluster_sanqier'
LIMIT 10;
```

#### æ£€æŸ¥å„è¡¨æ•°æ®é‡
```sql
-- å„è¡¨æ•°æ®ç»Ÿè®¡
SELECT 'spark_applications' AS table_name, COUNT(*) AS cnt FROM meta.spark_applications WHERE dt = '2025-11-26'
UNION ALL
SELECT 'spark_jobs', COUNT(*) FROM meta.spark_jobs WHERE dt = '2025-11-26'
UNION ALL
SELECT 'spark_stages', COUNT(*) FROM meta.spark_stages WHERE dt = '2025-11-26'
UNION ALL
SELECT 'spark_executors', COUNT(*) FROM meta.spark_executors WHERE dt = '2025-11-26'
UNION ALL
SELECT 'spark_parser_status', COUNT(*) FROM meta.spark_parser_status WHERE dt = '2025-11-26';
```

#### æ£€æŸ¥è§£æçŠ¶æ€
```sql
-- è§£ææˆåŠŸç‡
SELECT 
    status,
    COUNT(*) AS cnt,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) AS pct
FROM meta.spark_parser_status
WHERE dt = '2025-11-26'
GROUP BY status;
```

### 6.3 æ•°æ®è´¨é‡éªŒè¯

```sql
-- 1. æ£€æŸ¥å¿…å¡«å­—æ®µæ˜¯å¦ä¸ºç©º
SELECT COUNT(*) AS null_count
FROM meta.spark_applications
WHERE dt = '2025-11-26' 
  AND (app_id IS NULL OR cluster_name IS NULL);
-- é¢„æœŸ: 0

-- 2. æ£€æŸ¥æ—¶é—´é€»è¾‘æ˜¯å¦æ­£ç¡®
SELECT COUNT(*) AS invalid_time_count
FROM meta.spark_applications
WHERE dt = '2025-11-26' 
  AND start_time > end_time;
-- é¢„æœŸ: 0

-- 3. æ£€æŸ¥é‡å¤æ•°æ®
SELECT cluster_name, app_id, COUNT(*) AS cnt
FROM meta.spark_applications
WHERE dt = '2025-11-26'
GROUP BY cluster_name, app_id
HAVING COUNT(*) > 1;
-- é¢„æœŸ: æ— ç»“æœ

-- 4. æ£€æŸ¥åº”ç”¨çŠ¶æ€åˆ†å¸ƒ
SELECT status, COUNT(*) AS cnt
FROM meta.spark_applications
WHERE dt = '2025-11-26'
GROUP BY status;
-- é¢„æœŸçŠ¶æ€: FINISHED, FAILED, KILLED, RUNNING
```

### 6.4 è¯Šæ–­å»ºè®®éªŒè¯

```sql
-- æŸ¥çœ‹è¯Šæ–­å»ºè®®
SELECT app_id, rule_desc, severity, suggestion
FROM meta.spark_diagnosis
WHERE dt = '2025-11-26'
ORDER BY 
  CASE severity 
    WHEN 'CRITICAL' THEN 1
    WHEN 'WARNING' THEN 2
    ELSE 3
  END
LIMIT 20;
```

---

## 7. æ³¨æ„äº‹é¡¹

### 7.1 æµ‹è¯•å‰å‡†å¤‡

1. **ç¡®è®¤EventLogå­˜åœ¨**
   ```bash
   # æ£€æŸ¥ç›®æ ‡æ—¥æœŸæ˜¯å¦æœ‰EventLogæ–‡ä»¶
   hadoop fs -ls hdfs://beh006/var/log/hadoop-spark/ | grep "2025-11-26" | head -5
   ```

2. **é¿å…ä¸ç”Ÿäº§ä»»åŠ¡å†²çª**
   - æµ‹è¯•æ—¶é€‰æ‹©ä½å³°æœŸ
   - å°è§„æ¨¡æµ‹è¯•å…ˆç”¨å°‘é‡Executor

3. **å¤‡ä»½é…ç½®æ–‡ä»¶**
   ```bash
   cp config.yaml config.yaml.bak
   ```

### 7.2 æµ‹è¯•æœŸé—´

1. **å®æ—¶ç›‘æ§ä»»åŠ¡**
   - é€šè¿‡YARN UIæŸ¥çœ‹ä»»åŠ¡çŠ¶æ€: `http://your-yarn-rm:8088`
   - æŸ¥çœ‹Driver/Executoræ—¥å¿—

2. **èµ„æºç›‘æ§**
   - ç›‘æ§é›†ç¾¤CPU/å†…å­˜ä½¿ç”¨ç‡
   - ç¡®ä¿ä¸å½±å“å…¶ä»–ä»»åŠ¡

### 7.3 æµ‹è¯•åæ¸…ç†

```sql
-- å¦‚éœ€æ¸…ç†æµ‹è¯•æ•°æ®
ALTER TABLE meta.spark_applications DROP IF EXISTS PARTITION (dt='2025-11-26');
ALTER TABLE meta.spark_jobs DROP IF EXISTS PARTITION (dt='2025-11-26');
ALTER TABLE meta.spark_stages DROP IF EXISTS PARTITION (dt='2025-11-26');
ALTER TABLE meta.spark_executors DROP IF EXISTS PARTITION (dt='2025-11-26');
ALTER TABLE meta.spark_diagnosis DROP IF EXISTS PARTITION (dt='2025-11-26');
ALTER TABLE meta.spark_parser_status DROP IF EXISTS PARTITION (dt='2025-11-26');
ALTER TABLE meta.spark_sql_executions DROP IF EXISTS PARTITION (dt='2025-11-26');
ALTER TABLE meta.spark_configs DROP IF EXISTS PARTITION (dt='2025-11-26');
```

### 7.4 é‡è·‘æ•°æ®ï¼Œè­¬å¦‚å‰æœŸé€»è¾‘é—®é¢˜ï¼Œå¾…bugä¿®æ­£åï¼Œéœ€è¦é‡è·‘æ•°æ®

- æ‰§è¡Œpre_rerun.sh è„šæœ¬ï¼Œåˆ é™¤è¡¨é‡Œå¯¹åº”æ—¥æœŸ/é›†ç¾¤çš„åˆ†åŒº
- è‹¥åªæƒ³è¡¥å¤„ç†å¤±è´¥æ–‡ä»¶è€Œä¸å…¨é‡é‡è·‘ï¼Œå¯ä»¥åªåˆ é™¤ spark_parser_status ä¸­ status='FAILED' çš„è®°å½•æˆ–å¯¹åº”åˆ†åŒºã€‚


### 7.5 å®‰å…¨æ³¨æ„äº‹é¡¹

- âš ï¸ ä¸è¦åœ¨ç”Ÿäº§ç¯å¢ƒç›´æ¥æµ‹è¯•DELETE/DROPæ“ä½œ
- âš ï¸ æµ‹è¯•å®Œæˆåæ¢å¤é…ç½®æ–‡ä»¶
- âš ï¸ å¤§è§„æ¨¡æµ‹è¯•å‰å…ˆå°è§„æ¨¡éªŒè¯

---

## 8. å¸¸è§é—®é¢˜

### Q1: ä»»åŠ¡æäº¤å¤±è´¥ "é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"

**åŸå› ï¼š** config.yaml æ–‡ä»¶æœªåˆ›å»º

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
cp config.yaml.example config.yaml
# ç¼–è¾‘é…ç½®æ–‡ä»¶
vim config.yaml
```

### Q2: è¿æ¥HDFSå¤±è´¥

**åŸå› ï¼š** HDFSåœ°å€é…ç½®é”™è¯¯æˆ–ç½‘ç»œä¸é€š

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# 1. æ£€æŸ¥HDFSåœ°å€
hdfs dfs -ls hdfs://beh006/

# 2. æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„event_log_dir
grep event_log_dir config.yaml
```

### Q3: Hiveè¡¨ä¸å­˜åœ¨

**åŸå› ï¼š** æœªæ‰§è¡Œå»ºè¡¨SQL

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
hive -f create_hive_tables.sql
```

### Q4: è§£ææ•°æ®ä¸º0æ¡

**åŸå› ï¼š** 
1. EventLogç›®å½•è·¯å¾„é”™è¯¯
2. ç›®æ ‡æ—¥æœŸæ— EventLogæ–‡ä»¶
3. æ–‡ä»¶è¿‡æ»¤è§„åˆ™å¤ªä¸¥æ ¼

**æ’æŸ¥æ­¥éª¤ï¼š**
```bash
# 1. æ£€æŸ¥é…ç½®çš„EventLogç›®å½•
hadoop fs -ls hdfs://beh006/var/log/hadoop-spark/

# 2. æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦ç¬¦åˆè§„åˆ™ï¼ˆä»¥application_å¼€å¤´ï¼‰
hadoop fs -ls hdfs://beh006/var/log/hadoop-spark/ | grep "application_"

# 3. æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ˜¯å¦åœ¨ç›®æ ‡æ—¥æœŸ
hadoop fs -ls hdfs://beh006/var/log/hadoop-spark/ | head -20
```

### Q5: ä»»åŠ¡æ‰§è¡Œè¶…æ—¶

**åŸå› ï¼š** æ–‡ä»¶æ•°é‡å¤šä½†èµ„æºé…ç½®ä¸è¶³

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# å¢åŠ Executoræ•°é‡å’Œå¹¶è¡Œåº¦
NUM_EXECUTORS=200 PARALLELISM=2000 ./submit_parser.sh cluster_sanqier 2025-11-26
```

### Q6: å†…å­˜æº¢å‡º (OOM)

**åŸå› ï¼š** å•ä¸ªEventLogæ–‡ä»¶è¿‡å¤§æˆ–å†…å­˜é…ç½®ä¸è¶³

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# å¢åŠ Executorå†…å­˜
EXECUTOR_MEMORY=16g EXECUTOR_MEMORY_OVERHEAD=3g ./submit_parser.sh cluster_sanqier 2025-11-26
```

### Q7: æ•°æ®æœ‰é‡å¤

**åŸå› ï¼š** ä»»åŠ¡è¢«é‡å¤æ‰§è¡Œä½†å†™å…¥æ¨¡å¼é…ç½®ä¸ºappend

**è§£å†³æ–¹æ¡ˆï¼š**
1. ç¡®è®¤é…ç½®æ–‡ä»¶ä¸­ `write_mode: "overwrite"`
2. æ‰‹åŠ¨æ¸…ç†é‡å¤åˆ†åŒºåé‡è·‘

---

## ğŸ“ é—®é¢˜åé¦ˆ

å¦‚é‡åˆ°æœ¬æ–‡æ¡£æœªæ¶µç›–çš„é—®é¢˜ï¼Œè¯·æ”¶é›†ä»¥ä¸‹ä¿¡æ¯ååé¦ˆï¼š

1. æ‰§è¡Œçš„å‘½ä»¤
2. å®Œæ•´çš„é”™è¯¯æ—¥å¿—
3. config.yaml é…ç½®ï¼ˆè„±æ•åï¼‰
4. ç¯å¢ƒä¿¡æ¯ï¼ˆSpark/Hadoopç‰ˆæœ¬ç­‰ï¼‰
5. YARN Application IDï¼ˆå¦‚æœ‰ï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬ï¼š** v1.0  
**æœ€åæ›´æ–°ï¼š** 2025-12-07

