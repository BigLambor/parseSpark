# å¿«é€Ÿä¸Šæ‰‹æŒ‡å—

æœ¬æ–‡æ¡£å¸®åŠ©æ‚¨åœ¨10åˆ†é’Ÿå†…å®Œæˆç¬¬ä¸€æ¬¡Spark EventLogè§£æã€‚

## 1. ç¯å¢ƒæ£€æŸ¥

ç¡®ä¿æ‚¨çš„ç¯å¢ƒæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š

```bash
# æ£€æŸ¥Sparkç‰ˆæœ¬
spark-submit --version  # éœ€è¦ >= 3.0.0

# æ£€æŸ¥Pythonç‰ˆæœ¬
python --version  # éœ€è¦ >= 3.7

# æ£€æŸ¥Hadoopå®¢æˆ·ç«¯
hadoop version  # éœ€è¦ >= 3.0.0

# æ£€æŸ¥HDFSè¿æ¥
hadoop fs -ls /  # ç¡®ä¿èƒ½è®¿é—®HDFS

# æ£€æŸ¥Hive
hive --version  # éœ€è¦ >= 3.0.0
```

## 2. å…‹éš†é¡¹ç›®

```bash
cd /opt/apps
git clone <your-repo-url> spark-eventlog-parser
cd spark-eventlog-parser
```

## 3. é…ç½®æ–‡ä»¶

å¤åˆ¶é…ç½®æ¨¡æ¿å¹¶ä¿®æ”¹ï¼š

```bash
cp config.yaml.example config.yaml
vim config.yaml
```

**æœ€å°åŒ–é…ç½®ï¼ˆä»…éœ€ä¿®æ”¹è¿™3é¡¹ï¼‰ï¼š**

```yaml
hdfs:
  clusters:
    cluster1:
      event_log_dir: "/spark-logs"  # ä¿®æ”¹ä¸ºä½ çš„EventLogç›®å½•

hive:
  database: "meta"  # ä¿®æ”¹ä¸ºä½ çš„Hiveåº“å

parser:
  parse_parallelism: 2000  # æ ¹æ®é›†ç¾¤è§„æ¨¡è°ƒæ•´
```

## 4. åˆ›å»ºHiveè¡¨

```bash
# æ–¹å¼1ï¼šç›´æ¥æ‰§è¡ŒSQLæ–‡ä»¶
hive -f create_hive_tables.sql

# æ–¹å¼2ï¼šé€šè¿‡Beeline
beeline -u "jdbc:hive2://hive-server:10000" -f create_hive_tables.sql
```

éªŒè¯è¡¨æ˜¯å¦åˆ›å»ºæˆåŠŸï¼š

```bash
hive -e "SHOW TABLES IN meta LIKE 'spark_*'"
```

åº”è¯¥çœ‹åˆ°ï¼š
- spark_applications
- spark_jobs
- spark_stages
- spark_executors
- spark_diagnosis (å¯é€‰)

## 5. å°è§„æ¨¡æµ‹è¯•

å…ˆç”¨å°‘é‡æ•°æ®æµ‹è¯•ï¼ˆ1000ä¸ªæ–‡ä»¶ï¼‰ï¼š

```bash
# è®¾ç½®èµ„æºé…ç½®
export NUM_EXECUTORS=10
export EXECUTOR_MEMORY=4g
export PARALLELISM=100

# æäº¤ä»»åŠ¡ï¼ˆè§£ææ˜¨å¤©ï¼‰
./submit_parser.sh cluster1
```

**é¢„æœŸè¾“å‡ºï¼š**

```
==========================================
Spark EventLogè§£æä»»åŠ¡æäº¤ (PySpark)
==========================================
é›†ç¾¤åç§°: cluster1
ç›®æ ‡æ—¥æœŸ: 2024-01-14
...
ä»»åŠ¡æäº¤æˆåŠŸï¼
==========================================
```

## 6. æŸ¥çœ‹æ‰§è¡ŒçŠ¶æ€

### æ–¹å¼1ï¼šYARN UI

è®¿é—®ï¼š`http://your-yarn-rm:8088`

æ‰¾åˆ°åä¸º `SparkEventLogParser-cluster1-2024-01-14` çš„ä»»åŠ¡ã€‚

### æ–¹å¼2ï¼šSpark History Server

è®¿é—®ï¼š`http://your-spark-history:18080`

### æ–¹å¼3ï¼šæŸ¥çœ‹æ—¥å¿—

```bash
# è·å–Application ID
yarn application -list -appStates RUNNING | grep SparkEventLogParser

# æŸ¥çœ‹æ—¥å¿—
yarn logs -applicationId <application_id>
```

## 7. éªŒè¯ç»“æœ

ä»»åŠ¡å®Œæˆåï¼ŒæŸ¥è¯¢Hiveè¡¨ï¼š

```sql
-- æŸ¥çœ‹åº”ç”¨æ•°é‡
SELECT COUNT(*) as app_count
FROM meta.spark_applications
WHERE dt = '2024-01-14' AND cluster_name = 'cluster1';

-- æŸ¥çœ‹çŠ¶æ€åˆ†å¸ƒ
SELECT status, COUNT(*) as cnt
FROM meta.spark_applications
WHERE dt = '2024-01-14' AND cluster_name = 'cluster1'
GROUP BY status;

-- æŸ¥çœ‹è§£æçš„ç¬¬ä¸€ä¸ªåº”ç”¨
SELECT app_id, app_name, user, duration_ms, status
FROM meta.spark_applications
WHERE dt = '2024-01-14' AND cluster_name = 'cluster1'
LIMIT 1;
```

**æœŸæœ›ç»“æœï¼š**

- app_count > 0 ï¼ˆå¦‚æœæœ‰ä»»åŠ¡è¿è¡Œï¼‰
- çœ‹åˆ° FINISHED / FAILED / KILLED ç­‰çŠ¶æ€
- æ•°æ®å­—æ®µå®Œæ•´ï¼ˆæ— å¤§é‡NULLï¼‰

## 8. å…¨é‡è¿è¡Œ

å°è§„æ¨¡æµ‹è¯•æˆåŠŸåï¼Œä½¿ç”¨ç”Ÿäº§é…ç½®ï¼š

```bash
# æ¢å¤é»˜è®¤é…ç½®
unset NUM_EXECUTORS
unset EXECUTOR_MEMORY
unset PARALLELISM

# æäº¤å…¨é‡ä»»åŠ¡
./submit_parser.sh cluster1
```

**é¢„è®¡è€—æ—¶ï¼š**

- 1ä¸‡æ–‡ä»¶ï¼š5-10åˆ†é’Ÿ
- 10ä¸‡æ–‡ä»¶ï¼š15-30åˆ†é’Ÿ
- 30ä¸‡æ–‡ä»¶ï¼š30-60åˆ†é’Ÿ

## 9. è®¾ç½®å®šæ—¶è°ƒåº¦

### æ–¹å¼1ï¼šCrontabï¼ˆç®€å•ï¼‰

```bash
# ç¼–è¾‘crontab
crontab -e

# æ·»åŠ å®šæ—¶ä»»åŠ¡ï¼ˆæ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œï¼‰
0 2 * * * cd /opt/apps/spark-eventlog-parser && ./submit_parser.sh cluster1 >> /var/log/spark_parser.log 2>&1
```

### æ–¹å¼2ï¼šAirflowï¼ˆæ¨èï¼‰

å‚è€ƒ `README.md` ä¸­çš„Airflowé…ç½®ã€‚

## 10. ç›‘æ§å’Œå‘Šè­¦

### æŸ¥çœ‹è§£æç»Ÿè®¡

```sql
-- æ¯æ—¥è§£ææ•°é‡è¶‹åŠ¿
SELECT dt, COUNT(*) as app_count
FROM meta.spark_applications
WHERE dt >= DATE_SUB(CURRENT_DATE, 7)
  AND cluster_name = 'cluster1'
GROUP BY dt
ORDER BY dt DESC;

-- å¤±è´¥ç‡ç»Ÿè®¡
SELECT 
    dt,
    SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) as failed_count,
    COUNT(*) as total_count,
    ROUND(SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as failure_rate
FROM meta.spark_applications
WHERE dt >= DATE_SUB(CURRENT_DATE, 7)
  AND cluster_name = 'cluster1'
GROUP BY dt
ORDER BY dt DESC;
```

## å¸¸è§é—®é¢˜

### Q1: æ‰¾ä¸åˆ°EventLogç›®å½•

**é”™è¯¯ä¿¡æ¯ï¼š** `FileNotFoundError: EventLogç›®å½•ä¸å­˜åœ¨`

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ£€æŸ¥EventLogç›®å½•é…ç½®
grep event_log_dir config.yaml

# åˆ—å‡ºHDFSä¸Šçš„EventLogç›®å½•
hadoop fs -ls /spark-logs

# å¦‚æœè·¯å¾„ä¸åŒï¼Œä¿®æ”¹config.yamlä¸­çš„event_log_dir
```

### Q2: æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶

**é”™è¯¯ä¿¡æ¯ï¼š** `æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶ï¼Œé€€å‡º`

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ£€æŸ¥ç›®æ ‡æ—¥æœŸæ˜¯å¦æœ‰æ–‡ä»¶
hadoop fs -ls /spark-logs/2024-01-14/

# å¦‚æœæ²¡æœ‰æ—¥æœŸå­ç›®å½•ï¼Œæ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
hadoop fs -ls /spark-logs/ | grep 2024-01-14
```

### Q3: Hiveè¡¨ä¸å­˜åœ¨

**é”™è¯¯ä¿¡æ¯ï¼š** `Table or view 'spark_applications' not found`

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# ç¡®è®¤è¡¨æ˜¯å¦å­˜åœ¨
hive -e "SHOW TABLES IN meta LIKE 'spark_*'"

# å¦‚æœä¸å­˜åœ¨ï¼Œæ‰§è¡Œå»ºè¡¨SQL
hive -f create_hive_tables.sql

# æ£€æŸ¥æ˜¯å¦æˆåŠŸ
hive -e "DESC meta.spark_applications"
```

### Q4: å†…å­˜ä¸è¶³

**é”™è¯¯ä¿¡æ¯ï¼š** `ExecutorLostFailure` æˆ– `OutOfMemoryError`

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# å¢åŠ executorå†…å­˜
EXECUTOR_MEMORY=16g ./submit_parser.sh cluster1

# æˆ–è€…å‡å°‘executoræ•°é‡
NUM_EXECUTORS=100 ./submit_parser.sh cluster1
```

## ä¸‹ä¸€æ­¥

- é˜…è¯» [å®Œæ•´æ–‡æ¡£](README.md) äº†è§£æ›´å¤šåŠŸèƒ½
- æŸ¥çœ‹ [è®¾è®¡æ–‡æ¡£](Sparkä½œä¸šè§£ææ–¹æ¡ˆè®¾è®¡.md) äº†è§£æ¶æ„ç»†èŠ‚
- å‚è€ƒ [ä¼˜åŒ–æ€»ç»“](æ–¹æ¡ˆä¼˜åŒ–æ€»ç»“.md) è¿›è¡Œæ€§èƒ½è°ƒä¼˜
- é…ç½®ç›‘æ§å‘Šè­¦ï¼ˆPrometheus + Grafanaï¼‰

## è·å–å¸®åŠ©

é‡åˆ°é—®é¢˜ï¼Ÿ

1. æŸ¥çœ‹ [æ•…éšœæ’æŸ¥](README.md#æ•…éšœæ’æŸ¥) ç« èŠ‚
2. æœç´¢é¡¹ç›®Issues
3. è”ç³»ç»´æŠ¤å›¢é˜Ÿ

---

**æ­å–œï¼** æ‚¨å·²å®Œæˆç¬¬ä¸€æ¬¡Spark EventLogè§£æ ğŸ‰

