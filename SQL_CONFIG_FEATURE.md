# SQLè¯­å¥å’Œä¼šè¯å‚æ•°è·å–åŠŸèƒ½è¯´æ˜

## åŠŸèƒ½æ¦‚è¿°

æœ¬æ¬¡æ›´æ–°å®ç°äº†ä»Spark EventLogä¸­æå–**SQLè¯­å¥**å’Œ**ä¼šè¯å‚æ•°è®¾ç½®**çš„åŠŸèƒ½ï¼Œæ‰©å±•äº†åŸæœ‰çš„è§£æèƒ½åŠ›ã€‚

## âœ… å®ç°çš„åŠŸèƒ½

### 1. SQLè¯­å¥æå–

- **æ”¯æŒçš„äº‹ä»¶ç±»å‹**ï¼š
  - `SparkListenerSQLExecutionStart` - SQLæ‰§è¡Œå¼€å§‹
  - `SparkListenerSQLExecutionEnd` - SQLæ‰§è¡Œç»“æŸ

- **æå–çš„ä¿¡æ¯**ï¼š
  - SQLè¯­å¥æ–‡æœ¬ï¼ˆ`sql_text`ï¼‰
  - SQLæè¿°ï¼ˆ`description`ï¼‰
  - ç‰©ç†æ‰§è¡Œè®¡åˆ’æè¿°ï¼ˆ`physical_plan_description`ï¼‰
  - æ‰§è¡Œå¼€å§‹/ç»“æŸæ—¶é—´
  - æ‰§è¡ŒçŠ¶æ€ï¼ˆSUCCEEDED/FAILEDï¼‰
  - é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
  - å…³è”çš„Job IDs

### 2. ä¼šè¯å‚æ•°æå–

- **æ”¯æŒçš„äº‹ä»¶ç±»å‹**ï¼š
  - `SparkListenerEnvironmentUpdate` - ç¯å¢ƒæ›´æ–°äº‹ä»¶

- **æå–çš„å‚æ•°ç±»åˆ«**ï¼š
  - **Sparké…ç½®å‚æ•°** (`spark.*`) - æ‰€æœ‰spark.confè®¾ç½®çš„å‚æ•°
  - **ç³»ç»Ÿå±æ€§** (`system.*`) - JVMç³»ç»Ÿå±æ€§
  - **Javaå±æ€§** - Javaç‰ˆæœ¬ã€Java Homeç­‰

## ğŸ“Š æ–°å¢Hiveè¡¨

### 1. `spark_sql_executions` è¡¨

å­˜å‚¨SQLæ‰§è¡Œè®°å½•ï¼š

```sql
CREATE EXTERNAL TABLE spark_sql_executions (
    cluster_name STRING,
    app_id STRING,
    execution_id INT,
    sql_text STRING,              -- SQLè¯­å¥æ–‡æœ¬
    description STRING,
    physical_plan_description STRING,
    start_time BIGINT,
    end_time BIGINT,
    duration_ms BIGINT,
    job_ids STRING,              -- JSONæ•°ç»„å­—ç¬¦ä¸²
    status STRING,
    error_message STRING,
    create_time TIMESTAMP
) PARTITIONED BY (dt STRING);
```

### 2. `spark_configs` è¡¨

å­˜å‚¨Sparké…ç½®å‚æ•°ï¼š

```sql
CREATE EXTERNAL TABLE spark_configs (
    cluster_name STRING,
    app_id STRING,
    config_key STRING,           -- é…ç½®é”®
    config_value STRING,         -- é…ç½®å€¼
    config_category STRING,      -- spark/system/java
    create_time TIMESTAMP
) PARTITIONED BY (dt STRING);
```

## ğŸ”§ ä»£ç å˜æ›´

### æ–°å¢æ–‡ä»¶

1. **`models/sql_metrics.py`** - SQLå’Œé…ç½®æ•°æ®æ¨¡å‹
   - `SQLMetrics` - SQLæ‰§è¡ŒæŒ‡æ ‡æ¨¡å‹
   - `SparkConfigMetrics` - Sparké…ç½®æŒ‡æ ‡æ¨¡å‹

2. **`example_sql_queries.sql`** - SQLæŸ¥è¯¢ç¤ºä¾‹

### ä¿®æ”¹æ–‡ä»¶

1. **`parser/event_parser.py`**
   - æ‰©å±•`ApplicationState`ç±»ï¼Œæ·»åŠ SQLå’Œé…ç½®å­˜å‚¨
   - æ·»åŠ `to_sql_metrics()`å’Œ`to_config_metrics()`æ–¹æ³•
   - æ·»åŠ `SparkListenerSQLExecutionStart/End`äº‹ä»¶å¤„ç†
   - æ·»åŠ `SparkListenerEnvironmentUpdate`äº‹ä»¶å¤„ç†

2. **`create_hive_tables.sql`**
   - æ·»åŠ `spark_sql_executions`è¡¨å®šä¹‰
   - æ·»åŠ `spark_configs`è¡¨å®šä¹‰

3. **`parser/config_loader.py`**
   - æ·»åŠ æ–°è¡¨åé…ç½®ï¼š`sql_executions`ã€`spark_configs`

4. **`parser/hive_writer.py`**
   - æ·»åŠ `write_sql_executions()`æ–¹æ³•
   - æ·»åŠ `write_spark_configs()`æ–¹æ³•
   - æ›´æ–°`write_all()`æ–¹æ³•ï¼ŒåŒ…å«æ–°æ•°æ®å†™å…¥

5. **`parse_spark_logs.py`**
   - æ›´æ–°è§£æç»“æœï¼ŒåŒ…å«SQLå’Œé…ç½®æ•°æ®
   - æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…å«SQLå’Œé…ç½®è®¡æ•°

## ğŸ“ ä½¿ç”¨æ–¹æ³•

### 1. åˆ›å»ºæ–°è¡¨

æ‰§è¡Œæ›´æ–°åçš„å»ºè¡¨SQLï¼š

```bash
hive -f create_hive_tables.sql
```

### 2. è¿è¡Œè§£æç¨‹åº

è§£æç¨‹åºä¼šè‡ªåŠ¨æå–SQLå’Œé…ç½®ä¿¡æ¯ï¼Œæ— éœ€é¢å¤–é…ç½®ï¼š

```bash
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --conf spark.app.cluster_name=cluster1 \
  --conf spark.app.target_date=2024-01-15 \
  parse_spark_logs.py
```

### 3. æŸ¥è¯¢SQLæ‰§è¡Œè®°å½•

```sql
-- æŸ¥çœ‹æ‰€æœ‰SQLæ‰§è¡Œ
SELECT * FROM meta.spark_sql_executions
WHERE dt = '2024-01-15' AND cluster_name = 'cluster1'
ORDER BY start_time DESC;

-- æŸ¥æ‰¾å¤±è´¥çš„SQL
SELECT sql_text, error_message 
FROM meta.spark_sql_executions
WHERE dt = '2024-01-15' 
  AND status = 'FAILED';
```

### 4. æŸ¥è¯¢é…ç½®å‚æ•°

```sql
-- æŸ¥çœ‹åº”ç”¨çš„Sparké…ç½®
SELECT config_key, config_value
FROM meta.spark_configs
WHERE dt = '2024-01-15' 
  AND app_id = 'application_1234567890_0001'
  AND config_category = 'spark'
ORDER BY config_key;
```

æ›´å¤šæŸ¥è¯¢ç¤ºä¾‹è¯·å‚è€ƒ `example_sql_queries.sql`ã€‚

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. å…¼å®¹æ€§

- SQLäº‹ä»¶å¤„ç†å…¼å®¹ä¸åŒSparkç‰ˆæœ¬çš„å­—æ®µå
- å¦‚æœEventLogä¸­æ²¡æœ‰SQLäº‹ä»¶ï¼Œç›¸å…³è¡¨å°†ä¸ºç©ºï¼ˆä¸å½±å“å…¶ä»–æ•°æ®ï¼‰

### 2. æ•°æ®é‡

- SQLæ‰§è¡Œè®°å½•ï¼šæ¯ä¸ªSpark SQLåº”ç”¨å¯èƒ½æœ‰å¤šæ¡SQLæ‰§è¡Œè®°å½•
- é…ç½®å‚æ•°ï¼šæ¯ä¸ªåº”ç”¨å¯èƒ½æœ‰æ•°ç™¾æ¡é…ç½®è®°å½•ï¼ˆsparké…ç½®è¾ƒå¤šï¼‰

### 3. SQLæ–‡æœ¬é•¿åº¦

- SQLæ–‡æœ¬å¯èƒ½å¾ˆé•¿ï¼ŒæŸ¥è¯¢æ—¶å»ºè®®ä½¿ç”¨`LEFT(sql_text, 100)`æˆªæ–­æ˜¾ç¤º
- ç‰©ç†æ‰§è¡Œè®¡åˆ’æè¿°å¯èƒ½éå¸¸é•¿ï¼Œå»ºè®®æŒ‰éœ€æŸ¥è¯¢

### 4. å­—æ®µå…¼å®¹æ€§

ä¸åŒSparkç‰ˆæœ¬çš„EventLogå­—æ®µåå¯èƒ½ä¸åŒï¼Œä»£ç å·²åšå…¼å®¹å¤„ç†ï¼š
- `executionId` / `Execution ID` / `execution_id`
- `sqlText` / `SQL Text` / `sql`
- `physicalPlanDescription` / `Physical Plan Description`

## ğŸ” éªŒè¯æ–¹æ³•

### 1. æ£€æŸ¥æ•°æ®æ˜¯å¦å†™å…¥

```sql
-- æ£€æŸ¥SQLæ‰§è¡Œè®°å½•æ•°
SELECT COUNT(*) FROM meta.spark_sql_executions WHERE dt = '2024-01-15';

-- æ£€æŸ¥é…ç½®è®°å½•æ•°
SELECT COUNT(*) FROM meta.spark_configs WHERE dt = '2024-01-15';
```

### 2. æŠ½æ ·éªŒè¯

```sql
-- éšæœºæŸ¥çœ‹å‡ æ¡SQLè®°å½•
SELECT app_id, execution_id, LEFT(sql_text, 50) as sql_preview
FROM meta.spark_sql_executions
WHERE dt = '2024-01-15'
LIMIT 10;
```

## ğŸ“ˆ æ€§èƒ½å½±å“

- **è§£ææ€§èƒ½**ï¼šSQLå’Œé…ç½®äº‹ä»¶å¤„ç†å¼€é”€å¾ˆå°ï¼Œå¯¹æ•´ä½“è§£ææ€§èƒ½å½±å“å¯å¿½ç•¥
- **å­˜å‚¨ç©ºé—´**ï¼šSQLæ–‡æœ¬å’Œé…ç½®å‚æ•°ä¼šå¢åŠ å­˜å‚¨ç©ºé—´ï¼Œä½†é€šå¸¸ä¸ä¼šè¶…è¿‡åŸæœ‰æ•°æ®çš„10%
- **æŸ¥è¯¢æ€§èƒ½**ï¼šæ–°å¢è¡¨æŒ‰æ—¥æœŸåˆ†åŒºï¼ŒæŸ¥è¯¢æ€§èƒ½è‰¯å¥½

## ğŸ¯ åº”ç”¨åœºæ™¯

1. **SQLæ€§èƒ½åˆ†æ**ï¼šæ‰¾å‡ºæ‰§è¡Œæœ€æ…¢çš„SQLè¯­å¥
2. **SQLé”™è¯¯æ’æŸ¥**ï¼šæŸ¥çœ‹å¤±è´¥çš„SQLåŠå…¶é”™è¯¯ä¿¡æ¯
3. **é…ç½®å®¡è®¡**ï¼šæŸ¥çœ‹åº”ç”¨ä½¿ç”¨çš„Sparké…ç½®å‚æ•°
4. **é…ç½®ä¼˜åŒ–**ï¼šåˆ†æä¸åŒé…ç½®å¯¹æ€§èƒ½çš„å½±å“
5. **SQLå®¡è®¡**ï¼šè®°å½•æ‰€æœ‰æ‰§è¡Œçš„SQLè¯­å¥

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `example_sql_queries.sql` - SQLæŸ¥è¯¢ç¤ºä¾‹
- `create_hive_tables.sql` - Hiveè¡¨ç»“æ„å®šä¹‰
- `README.md` - é¡¹ç›®æ€»ä½“è¯´æ˜

